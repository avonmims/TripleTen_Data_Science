{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f81471",
   "metadata": {},
   "source": [
    "***KNOWN***\n",
    "\n",
    "// general info\n",
    "\n",
    "- Binary Classification Task\n",
    "- features = ['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "- target = ['Exited']\n",
    "- no external test set: 3:1:1 - training:valid:test - 60%:20%:20%\n",
    "\n",
    "// feature types\n",
    "\n",
    "- Categorical Features: ['Surname', 'Geography', 'Gender']\n",
    "- Numerical Features: ['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "- Target is Numerical\n",
    "\n",
    "// misc observations\n",
    "\n",
    "- Tenure column has 9091 / 10000 entries (-909) (1.1% missing)\n",
    "- Surname column contains entries with special characters: data['Surname'][9] = 'H?'\n",
    "- no duplicate CustomerId, 10000 different customers\n",
    "- 'IsActiveMemeber' not mutually exclusive with 'Exited'\n",
    "\n",
    "// class balance\n",
    "\n",
    "- 79.63% of customers have exited ('Exited' == 1)\n",
    "- 20.37% of customers have not exited ('Exited' == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ce17",
   "metadata": {},
   "source": [
    "***UNKNOWN***\n",
    "- model type? DecisionTreeClassifier, RandomForestClassifier, LogisticRegression\n",
    "- hyperparameters?\n",
    "- class balancing methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc211193",
   "metadata": {},
   "source": [
    "***OBJECTIVES***\n",
    "- F1 score: 0.59 against test set\n",
    "- Plot ROC and measure AUC-ROC\n",
    "- methods of balance to try: upsampling & class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cc55f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Churn.csv')\n",
    "\n",
    "#print(data.dtypes)\n",
    "#data.info(verbose=True)\n",
    "#display(data.head(10))\n",
    "#print(data['Surname'][9])\n",
    "#print(data.isna().sum())\n",
    "#print(data['CustomerId'].duplicated().sum())\n",
    "#print(data['Surname'].value_counts())\n",
    "#print(data[data['Surname'] == 'Smith'])\n",
    "#print(data.duplicated(subset='CustomerId').value_counts())\n",
    "#print(data[data['Tenure'].isna()])\n",
    "#print(data[(data['Tenure'] > 0) & (data['Tenure'] < 1)])\n",
    "#print(data[(data['Tenure'] < 1)])\n",
    "#print(data['Geography'].value_counts())\n",
    "\n",
    "data['Tenure_Missing'] = data['Tenure'].isnull().astype(int)\n",
    "data['Tenure'] = data['Tenure'].fillna(0)\n",
    "#print(data[data['Tenure_Missing'] == 1])\n",
    "#print(data.info(verbose=True))\n",
    "\n",
    "#print(data['Exited'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56835b",
   "metadata": {},
   "source": [
    "***data analysis & cleaning, adding surface-level observations to known & unknown***\n",
    "\n",
    "**We've done away with the missing values within the 'Tenure' column, replacing them with 0. Seeing as though 'Tenure' is measured in years, and only whole numbers, if there exists a customer with 11 months of loan history, are they rounded up to 1.0? or do they remain at 0.0? Or could the missing values have different implications on a per observation basis? Human error, new customer, bank system glitch, or has no active loan?**\n",
    "\n",
    "**In any case, I've decided to create a new column that saves the instances of 'Tenure' == NaN, as 'Tenure_Missing'**\n",
    "\n",
    "**Class balance is 79.63% negative, 20.37% positive. We NEED to consider this when training our model later, as predicting positive for every observation would yield a ~80% accuracy rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "959f9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# since our categorical entries are nominal, we will use OHE to prepare them\n",
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# separate features from target\n",
    "features = data_ohe.drop('Exited', axis=1)\n",
    "target = data_ohe['Exited']\n",
    "\n",
    "# 60% train set, 40% temporary set\n",
    "features_train, features_temp, target_train, target_temp = train_test_split(features, target, test_size=.4, random_state=12345)\n",
    "\n",
    "# 20% valid set, 20% test set\n",
    "features_valid , features_test, target_valid, target_test = train_test_split(features_temp, target_temp, test_size=.5, random_state=12345)\n",
    "\n",
    "# specify numeric features\n",
    "numeric = ['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "# tune scaler to training data features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "\n",
    "# apply scaling to numeric columns in our feature sets\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "#print(features_train.head())\n",
    "#print(features_valid.head())\n",
    "#print(features_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121af663",
   "metadata": {},
   "source": [
    "**Data preprocessing is completed, next steps are training a model without considerations to class imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4928d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.831', 'Recall': '0.498', 'Precision': '0.619', 'F1': '0.552'}\n",
      "Confusion Matrix: \n",
      " [[1454  128]\n",
      " [ 210  208]]\n",
      "\n",
      "Naive baseline accuracy of the validation set:  0.791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# training model on vanilla data, no upsampling, predicting target_valid using features_valid\n",
    "model = DecisionTreeClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "# custom function for extracting multiple score metrics\n",
    "def get_scores(target, predictions):\n",
    "    accuracy = accuracy_score(target, predictions)\n",
    "    recall = recall_score(target, predictions)\n",
    "    precision = precision_score(target, predictions)\n",
    "    f1 = f1_score(target, predictions)\n",
    "    return {'Accuracy': f'{accuracy:.3f}',\n",
    "            'Recall': f'{recall:.3f}',\n",
    "            'Precision': f'{precision:.3f}',\n",
    "            'F1': f'{f1:.3f}'}\n",
    "\n",
    "# capturing naive baseline accuracy of our test_valid set\n",
    "most_frequent = target_valid.mode()[0]\n",
    "baseline_accuracy = (target_valid == most_frequent).sum() / len(target_valid)\n",
    "\n",
    "print(get_scores(target_valid, predicted_valid))\n",
    "print(f'Confusion Matrix: \\n', confusion_matrix(target_valid, predicted_valid))\n",
    "print()\n",
    "print('Naive baseline accuracy of the validation set: ', baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b868c6e",
   "metadata": {},
   "source": [
    "**After calcuating naive baseline accuracy, we can see that the accuracy of our model (.831) is barely performing better than just predicting the most common observation (.791). Our recall of .49 tells us that our model, without upsampling to balance classes, is predicting alot of false negatives. While our precision score of .61 tells us that our model is having an easier time missing false positives, again because of the class weight imbalance. These two metrics give us an F1 score of .55, which again lets us know that our model is hardly performing better than guessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dbe787",
   "metadata": {},
   "source": [
    "***Model evaluation without class balancing is completed, scoring metrics are pulled, next step is to use two different weight balancing methods to see which one yields us the best model for our goal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "af29f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# upsample function creation\n",
    "def upsample(features, target, repeat):\n",
    "    \n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "# creating new, uspampled training sets, using a repeat value of 4 to get our class weight close to 50/50\n",
    "features_upsampled , target_upsampled = upsample(features_train, target_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48497a",
   "metadata": {},
   "source": [
    "**Upsampling completed, training model with new balanced training sets, displaying metric scores. Then applying hyperparameter (class_weight='balanced') to compare results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "39354203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced Decision Tree:  {'Accuracy': '0.831', 'Recall': '0.498', 'Precision': '0.619', 'F1': '0.552'}\n",
      "Upsampled Decision Tree:  {'Accuracy': '0.809', 'Recall': '0.572', 'Precision': '0.541', 'F1': '0.556'}\n",
      "Weighted Decision Tree:  {'Accuracy': '0.810', 'Recall': '0.574', 'Precision': '0.544', 'F1': '0.559'}\n"
     ]
    }
   ],
   "source": [
    "# preserved score metrics from model trained on imbalanced data\n",
    "print(f'Imbalanced Decision Tree: ', {'Accuracy': '0.831', 'Recall': '0.498', 'Precision': '0.619', 'F1': '0.552'})\n",
    "\n",
    "# training model on upsampled data, extracting score metrics\n",
    "model = DecisionTreeClassifier(random_state=12345)\n",
    "model.fit(features_upsampled, target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(f'Upsampled Decision Tree: ', get_scores(target_valid, predicted_valid))\n",
    "\n",
    "# training model on vanilla data, utilizing class weight balancing hyperparameter\n",
    "model = DecisionTreeClassifier(random_state=12345, class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(f'Weighted Decision Tree: ', get_scores(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d34ff5",
   "metadata": {},
   "source": [
    "**After training our model in 3 different \"environments\" (imbalanced, upsampled, weighted), we can see that utilizing the class weight balancing hyperparameter (class_weight='balanced') yields us the best results for this specific task**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86949494",
   "metadata": {},
   "source": [
    "***Next step is to iterate through differing models and parameters to find the best one for our task, with a desired F1 score of at least .59***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5493ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.809', 'Recall': '0.677', 'Precision': '0.534', 'F1': '0.597'}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1335  247]\n",
      " [ 135  283]]\n"
     ]
    }
   ],
   "source": [
    "# DECISION TREE TRAINING WITH SCORES\n",
    "\n",
    "best_model = None\n",
    "best_result = 0\n",
    "\n",
    "# loop to iterate through tree depth, saving best model and score\n",
    "# hashing out model training loops to save memory space\n",
    "\n",
    "#for depth in range(1, 31):\n",
    "    #model = DecisionTreeClassifier(random_state=12345, class_weight='balanced', max_depth=depth)\n",
    "    #model.fit(features_train, target_train)\n",
    "    #predicted_valid = model.predict(features_valid)\n",
    "    #score = f1_score(target_valid, predicted_valid)\n",
    "    #if score > best_result:\n",
    "        #best_model = model\n",
    "        #best_result = score\n",
    "\n",
    "#print('Best Model: ', best_model)\n",
    "#print('F1 score of best model: ', best_result)\n",
    "\n",
    "# resulting model from depth training\n",
    "model = DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(get_scores(target_valid, predicted_valid))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(target_valid, predicted_valid))\n",
    "\n",
    "# i looped through 2 more hyperparameters, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a3e11",
   "metadata": {},
   "source": [
    "**We've crossed the F1 score threshold with this model (0.597), and have made remarkable improvements compared to our model trained on imbalanced data. The confusion matrix also shows us that our model is catching more True Negatives & True Positives (1335 & 283), than it is catching False Negatives and False Positives (135 & 247)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78daec",
   "metadata": {},
   "source": [
    "***I will continue to train other models to see if we can get our scores any higher***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1f9a06b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.835', 'Recall': '0.548', 'Precision': '0.619', 'F1': '0.581'}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1441  141]\n",
      " [ 189  229]]\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST TRAINING WITH SCORES\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#best_model = None\n",
    "#best_result = 0\n",
    "\n",
    "# loop to iterate through forest depth and estimators count\n",
    "# hashing out model training for better notebook performance\n",
    "\n",
    "#for est in range(40, 101, 10):\n",
    "    #for depth in range(1, 31):\n",
    "        #model = RandomForestClassifier(random_state=12345, class_weight='balanced',\n",
    "                                       #n_estimators=est, max_depth=depth)\n",
    "        #model.fit(features_train, target_train)\n",
    "        #predicted_valid = model.predict(features_valid)\n",
    "        #score = f1_score(target_valid, predicted_valid)\n",
    "        #if score > best_result:\n",
    "            #best_model = model\n",
    "            #best_result = score\n",
    "\n",
    "#print(best_model, best_result)\n",
    "\n",
    "# resulting model from depth and estimators training\n",
    "model = RandomForestClassifier(class_weight='balanced', max_depth=40, n_estimators=90, random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(get_scores(target_valid, predicted_valid))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f78a7",
   "metadata": {},
   "source": [
    "**Our Random Forest Classification model scored slightly higher in recall, but took a noticeable hit to precision and F1 score. We will stick to the Decision Tree model for the time being, but Logistic Regression still has a chance to shine hopefully!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1e4bcd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.710', 'Recall': '0.617', 'Precision': '0.380', 'F1': '0.470'}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1161  421]\n",
      " [ 160  258]]\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION TRAINING WITH SCORES\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=12345, class_weight='balanced', solver='liblinear')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(get_scores(target_valid, predicted_valid))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc90a93",
   "metadata": {},
   "source": [
    "**Our Logistic Regression model scored significantly worse than even the Random Forest model, with a precision score of (0.380) and an F1 score of (0.470). All models were trained with the class weight hyperparameter, and made predictions on our validation set. The next step is to assess our Decision Tree model's performance on 'new data', our test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ee7f44ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier:  {'Accuracy': '0.809', 'Recall': '0.677', 'Precision': '0.534', 'F1': '0.597'}\n",
      "Random Forest Classifier:  {'Accuracy': '0.789', 'Recall': '0.682', 'Precision': '0.497', 'F1': '0.575'}\n",
      "Logistic Regression:  {'Accuracy': '0.710', 'Recall': '0.617', 'Precision': '0.380', 'F1': '0.470'}\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision Tree Classifier: ', {'Accuracy': '0.809', 'Recall': '0.677', 'Precision': '0.534', 'F1': '0.597'})\n",
    "print(f'Random Forest Classifier: ', {'Accuracy': '0.789', 'Recall': '0.682', 'Precision': '0.497', 'F1': '0.575'})\n",
    "print(f'Logistic Regression: ', {'Accuracy': '0.710', 'Recall': '0.617', 'Precision': '0.380', 'F1': '0.470'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605eb24c",
   "metadata": {},
   "source": [
    "**Next step is to use our model against the test set, with a desired F1 score of .59**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bc168aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.822', 'Recall': '0.643', 'Precision': '0.569', 'F1': '0.604'}\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1371  206]\n",
      " [ 151  272]]\n"
     ]
    }
   ],
   "source": [
    "# THRESHOLD TUNING & ASSESSING MODEL PERFORMANCE ON TEST SET\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "\n",
    "#prob = model.predict_proba(features_valid)[:,1]\n",
    "#best_score = 0\n",
    "#best_threshold = .5\n",
    "#for threshold in np.arange(0.2, 0.8, 0.01):\n",
    "    #predictions = prob > threshold\n",
    "    #score = f1_score(target_valid, predictions)\n",
    "    #if score > best_score:\n",
    "        #best_score = score\n",
    "        #best_threshold = threshold\n",
    "\n",
    "#print(best_score, best_threshold)\n",
    "\n",
    "best_threshold = 0.5700000000000003\n",
    "prob_test = model.predict_proba(features_test)[:,1]\n",
    "predicted_test_custom = prob_test > best_threshold\n",
    "\n",
    "print(get_scores(target_test, predicted_test_custom))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(target_test, predicted_test_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b508cd",
   "metadata": {},
   "source": [
    "**Before threshold tuning (default=.5), our F1 score came out to be (0.584) on the test set. By looping through threshold values we were able to find the best value (threshold=0.5700000000000003) suited for achieving our F1 score goals (0.604). Our trained model with a new threshold performs better on the test set, than the trained model with a default threshold performed on the validation set!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
