{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f81471",
   "metadata": {},
   "source": [
    "***KNOWN***\n",
    "\n",
    "// general info\n",
    "\n",
    "- Binary Classification Task\n",
    "- features = ['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "- target = ['Exited']\n",
    "- no external test set: 3:1:1 - training:valid:test - 60%:20%:20%\n",
    "\n",
    "// feature types\n",
    "\n",
    "- Categorical Features: ['Surname', 'Geography', 'Gender']\n",
    "- Numerical Features: ['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "- Target is Numerical\n",
    "\n",
    "// misc observations\n",
    "\n",
    "- Tenure column has 9091 / 10000 entries (-909) (1.1% missing)\n",
    "- Surname column contains entries with special characters: data['Surname'][9] = 'H?'\n",
    "- no duplicate CustomerId, 10000 different customers\n",
    "- 'IsActiveMemeber' not mutually exclusive with 'Exited'\n",
    "\n",
    "// class balance\n",
    "\n",
    "- 79.63% of customers have exited ('Exited' == 1)\n",
    "- 20.37% of customers have not exited ('Exited' == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067ce17",
   "metadata": {},
   "source": [
    "***UNKNOWN***\n",
    "- model type? DecisionTreeClassifier, RandomForestClassifier, LogisticRegression\n",
    "- hyperparameters?\n",
    "- class balancing methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc211193",
   "metadata": {},
   "source": [
    "***OBJECTIVES***\n",
    "- F1 score: 0.59 against test set\n",
    "- Plot ROC and measure AUC-ROC\n",
    "- methods of balance to try: upsampling & class_weight='balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Churn.csv')\n",
    "\n",
    "#print(data.dtypes)\n",
    "#data.info(verbose=True)\n",
    "#display(data.head(10))\n",
    "#print(data['Surname'][9])\n",
    "#print(data.isna().sum())\n",
    "#print(data['CustomerId'].duplicated().sum())\n",
    "#print(data['Surname'].value_counts())\n",
    "#print(data[data['Surname'] == 'Smith'])\n",
    "#print(data.duplicated(subset='CustomerId').value_counts())\n",
    "#print(data[data['Tenure'].isna()])\n",
    "#print(data[(data['Tenure'] > 0) & (data['Tenure'] < 1)])\n",
    "#print(data[(data['Tenure'] < 1)])\n",
    "#print(data['Geography'].value_counts())\n",
    "\n",
    "data['Tenure_Missing'] = data['Tenure'].isnull().astype(int)\n",
    "data['Tenure'] = data['Tenure'].fillna(0)\n",
    "#print(data[data['Tenure_Missing'] == 1])\n",
    "#print(data.info(verbose=True))\n",
    "\n",
    "#print(data['Exited'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56835b",
   "metadata": {},
   "source": [
    "***data analysis & cleaning, adding surface-level observations to known & unknown***\n",
    "\n",
    "**We've done away with the missing values within the 'Tenure' column, replacing them with 0. Seeing as though 'Tenure' is measured in years, and only whole numbers, if there exists a customer with 11 months of loan history, are they rounded up to 1.0? or do they remain at 0.0? Or could the missing values have different implications on a per observation basis? Human error, new customer, bank system glitch, or has no active loan?**\n",
    "\n",
    "**In any case, I've decided to create a new column that saves the instances of 'Tenure' == NaN, as 'Tenure_Missing'**\n",
    "\n",
    "**Class balance is 79.63% negative, 20.37% positive. We NEED to consider this when training our model later, as predicting positive for every observation would yield a ~80% accuracy rating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "959f9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# since our categorical entries are nominal, we will use OHE to prepare them\n",
    "data_ohe = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# separate features from target\n",
    "features = data_ohe.drop('Exited', axis=1)\n",
    "target = data_ohe['Exited']\n",
    "\n",
    "# 60% train set, 40% temporary set\n",
    "features_train, features_temp, target_train, target_temp = train_test_split(features, target, test_size=.4, random_state=12345)\n",
    "\n",
    "# 20% valid set, 20% test set\n",
    "features_valid , features_test, target_valid, target_test = train_test_split(features_temp, target_temp, test_size=.5, random_state=12345)\n",
    "\n",
    "# specify numeric features\n",
    "numeric = ['RowNumber', 'CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary']\n",
    "\n",
    "# tune scaler to training data features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "\n",
    "# apply scaling to numeric columns in our feature sets\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "#print(features_train.head())\n",
    "#print(features_valid.head())\n",
    "#print(features_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121af663",
   "metadata": {},
   "source": [
    "**Data preprocessing is completed, next steps are training a model without considerations to class imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4928d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.831', 'Recall': '0.498', 'Precision': '0.619', 'F1': '0.552'}\n",
      "Confusion Matrix: \n",
      " [[1454  128]\n",
      " [ 210  208]]\n",
      "\n",
      "Naive baseline accuracy of the validation set:  0.791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# training model on vanilla data, no upsampling, predicting target_valid using features_valid\n",
    "model = DecisionTreeClassifier(random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "# custom function for extracting multiple score metrics\n",
    "def get_scores(target, predictions):\n",
    "    accuracy = accuracy_score(target, predictions)\n",
    "    recall = recall_score(target, predictions)\n",
    "    precision = precision_score(target, predictions)\n",
    "    f1 = f1_score(target, predictions)\n",
    "    return {'Accuracy': f'{accuracy:.3f}',\n",
    "            'Recall': f'{recall:.3f}',\n",
    "            'Precision': f'{precision:.3f}',\n",
    "            'F1': f'{f1:.3f}'}\n",
    "\n",
    "# capturing naive baseline accuracy of our test_valid set\n",
    "most_frequent = target_valid.mode()[0]\n",
    "baseline_accuracy = (target_valid == most_frequent).sum() / len(target_valid)\n",
    "\n",
    "print(get_scores(target_valid, predicted_valid))\n",
    "print(f'Confusion Matrix: \\n', confusion_matrix(target_valid, predicted_valid))\n",
    "print()\n",
    "print('Naive baseline accuracy of the validation set: ', baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b868c6e",
   "metadata": {},
   "source": [
    "**After calcuating naive baseline accuracy, we can see that the accuracy of our model (.831) is barely performing better than just predicting the most common observation (.791). Our recall of .49 tells us that our model, without upsampling to balance classes, is predicting alot of false negatives. While our precision score of .61 tells us that our model is having an easier time missing false positives, again because of the class weight imbalance. These two metrics give us an F1 score of .55, which again lets us know that our model is hardly performing better than guessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dbe787",
   "metadata": {},
   "source": [
    "***Model evaluation without class balancing is completed, scoring metrics are pulled, next step is to use two different weight balancing methods to see which one yields us the best model for our goal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "af29f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# upsample function creation\n",
    "def upsample(features, target, repeat):\n",
    "    \n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=12345)\n",
    "\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "# creating new, uspampled training sets, using a repeat value of 4 to get our class weight close to 50/50\n",
    "features_upsampled , target_upsampled = upsample(features_train, target_train, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48497a",
   "metadata": {},
   "source": [
    "**Upsampling completed, training model with new balanced training sets, displaying metric scores. Then applying hyperparameter (class_weight='balanced') to compare results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "39354203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalanced Deccision Tree:  {'Accuracy': '0.831', 'Recall': '0.498', 'Precision': '0.619', 'F1': '0.552'}\n",
      "Upsampled Decision Tree:  {'Accuracy': '0.809', 'Recall': '0.572', 'Precision': '0.541', 'F1': '0.556'}\n",
      "Weighted Decision Tree:  {'Accuracy': '0.810', 'Recall': '0.574', 'Precision': '0.544', 'F1': '0.559'}\n"
     ]
    }
   ],
   "source": [
    "# preserved score metrics from model trained on imbalanced data\n",
    "print(f'Imbalanced Deccision Tree: ', {'Accuracy': '0.831', 'Recall': '0.498', 'Precision': '0.619', 'F1': '0.552'})\n",
    "\n",
    "# training model on upsampled data, extracting score metrics\n",
    "model = DecisionTreeClassifier(random_state=12345)\n",
    "model.fit(features_upsampled, target_upsampled)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(f'Upsampled Decision Tree: ', get_scores(target_valid, predicted_valid))\n",
    "\n",
    "# training model on vanilla data, utilizing class weight balancing hyperparameter\n",
    "model = DecisionTreeClassifier(random_state=12345, class_weight='balanced')\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(f'Weighted Decision Tree: ', get_scores(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d34ff5",
   "metadata": {},
   "source": [
    "**After training our model in 3 different \"environments\" (imbalanced, upsampled, weighted), we can see that utilizing the class weight balancing hyperparameter (class_weight='balanced') yields us the best results for this specific task**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86949494",
   "metadata": {},
   "source": [
    "***Next step is to iterate through differing models and parameters to find the best one for our task, with a desired F1 score of at least .59***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b5493ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': '0.809', 'Recall': '0.677', 'Precision': '0.534', 'F1': '0.597'}\n",
      "Confusion Matrix:\n",
      " [[1335  247]\n",
      " [ 135  283]]\n"
     ]
    }
   ],
   "source": [
    "# DECISION TREE TRAINING WITH SCORES\n",
    "\n",
    "#best_model = None\n",
    "#best_result = 0\n",
    "\n",
    "# loop to iterate through tree depth, saving best model and score\n",
    "# hashing out model training loops to save memory space\n",
    "\n",
    "#for depth in range(1, 11):\n",
    "    #model = DecisionTreeClassifier(random_state=12345, class_weight='balanced', max_depth=depth)\n",
    "    #model.fit(features_train, target_train)\n",
    "    #predicted_valid = model.predict(features_valid)\n",
    "    #score = f1_score(target_valid, predicted_valid)\n",
    "    #if score > best_result:\n",
    "        #best_model = model\n",
    "        #best_result = score\n",
    "\n",
    "#print('Best Model: ', best_model)\n",
    "#print('F1 score of best model: ', best_result)\n",
    "\n",
    "# resulting model from depth training\n",
    "model = DecisionTreeClassifier(class_weight='balanced', max_depth=5, random_state=12345)\n",
    "model.fit(features_train, target_train)\n",
    "predicted_valid = model.predict(features_valid)\n",
    "\n",
    "print(get_scores(target_valid, predicted_valid))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(target_valid, predicted_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a3e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
